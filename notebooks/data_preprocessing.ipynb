{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook covers data loading, cleaning, and preprocessing steps for the sentiment analysis project.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory\n",
    "parent_directory = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "if parent_directory not in sys.path:\n",
    "    sys.path.append(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from app.utils.preprocessing import clean_text, preprocess_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = preprocess_data(train_data)\n",
    "test_sentences, test_labels = preprocess_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: Counter({0: 12500, 1: 12500})\n",
      "Balanced dataset: Counter({0: 12500, 1: 12500})\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(train_labels)\n",
    "print(f\"Original dataset: {counter}\")\n",
    "\n",
    "# Create a balanced dataset by undersampling the majority class\n",
    "min_count = min(counter.values())\n",
    "balanced_train_sentences = []\n",
    "balanced_train_labels = []\n",
    "\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "for sentence, label in zip(train_sentences, train_labels):\n",
    "    if label == 1 and pos_count < min_count:\n",
    "        balanced_train_sentences.append(sentence)\n",
    "        balanced_train_labels.append(label)\n",
    "        pos_count += 1\n",
    "    elif label == 0 and neg_count < min_count:\n",
    "        balanced_train_sentences.append(sentence)\n",
    "        balanced_train_labels.append(label)\n",
    "        neg_count += 1\n",
    "\n",
    "print(f\"Balanced dataset: {Counter(balanced_train_labels)}\")\n",
    "\n",
    "train_sentences = balanced_train_sentences\n",
    "train_labels = balanced_train_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text data\n",
    "train_sentences = [clean_text(sent) for sent in train_sentences]\n",
    "test_sentences = [clean_text(sent) for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=120)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_save_path = os.path.join(parent_directory, \"app\\\\tokenizer\\\\tokenizer.pickle\")\n",
    "\n",
    "with open(tokenizer_save_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_directory = os.path.join(parent_directory, \"data\\\\processed\")\n",
    "\n",
    "# Save processed data\n",
    "np.save(os.path.join(processed_data_directory, \"train_padded.npy\"), train_padded)\n",
    "np.save(os.path.join(processed_data_directory, \"train_labels.npy\"), train_labels)\n",
    "np.save(os.path.join(processed_data_directory, \"test_padded.npy\"), test_padded)\n",
    "np.save(os.path.join(processed_data_directory, \"test_labels.npy\"), test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
